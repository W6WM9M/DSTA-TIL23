{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrJcGJ424Bfe",
        "outputId": "7cc87053-21a9-4f9f-e18e-6b1902048c48"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install timm"
      ],
      "metadata": {
        "id": "_D-S8bQOKJJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --folder https://drive.google.com/drive/folders/115l82GBgu6RETopB_Qy36hn4TVwXI2t5?usp=sharing"
      ],
      "metadata": {
        "id": "x78XXRejP_06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir \"/content/Datasets\"\n",
        "!mkdir \"/content/Datasets/images\"\n",
        "!mkdir \"/content/Datasets/images/test\"\n",
        "!unzip \"/content/Advanced/CV/Test.zip\" -d \"/content/Datasets/images/test\"\n",
        "\n",
        "!mkdir \"/content/Datasets/images/suspects\"\n",
        "!unzip \"/content/Advanced/CV/suspects.zip\" -d \"/content/Datasets/images/suspects\""
      ],
      "metadata": {
        "id": "U8YsV7EWKtkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7WzCYRk0KMOS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "ROOT = \".\" # Change this accordingly - tmp location"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "from transformers import  AutoImageProcessor, ResNetModel,ViTImageProcessor , ViTModel\n",
        "import timm\n",
        "from timm.data import resolve_data_config\n",
        "from timm.data.transforms_factory import create_transform\n",
        "\n",
        "class BGR2RGB:\n",
        "    def __call__(self, image):\n",
        "        return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "class SquarePad:\n",
        "    def __call__(self, image):\n",
        "        max_wh = max(image.shape[:2])\n",
        "        p_top, p_left = [(max_wh - s) // 2 for s in image.shape[:2]]\n",
        "        p_bottom, p_right = [max_wh - (s+pad) for s, pad in zip(image.shape[:2], [p_top,p_left])]\n",
        "        return cv2.copyMakeBorder(image, p_top, p_bottom, p_left, p_right, cv2.BORDER_CONSTANT, None, value = 0)\n",
        "\n",
        "def load_feature_extractor(model_name):\n",
        "    if model_name == \"resnet\":\n",
        "        model = ResNetModel.from_pretrained(\"microsoft/resnet-50\").to(device).eval()\n",
        "        transform = transforms.Compose([BGR2RGB(),\n",
        "            SquarePad(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "        \n",
        "        return model, transform \n",
        "    elif model_name == \"vit\":\n",
        "        processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k').to(device).eval()\n",
        "    \n",
        "        return model, processor\n",
        "\n",
        "    elif model_name == \"senet\":\n",
        "        model = timm.create_model('seresnet152d', pretrained=True,num_classes = 0).to(device).eval()\n",
        "        config = resolve_data_config({}, model=model)\n",
        "        transform = create_transform(**config)\n",
        "        return model, transform     \n",
        "\n",
        "class NN_Classifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(NN_Classifier, self).__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Dropout(p = 0.5),\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(p = 0.5),\n",
        "            nn.Linear(hidden_size, output_size)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        output = self.linear_relu_stack(x)\n",
        "        return output\n",
        "\n",
        "def signed_sqrt(x1, x2):\n",
        "    return torch.sign(x1*x2) * torch.sqrt(torch.abs(x1*x2))\n",
        "\n",
        "def combine_function(x1, x2):\n",
        "    return torch.concatenate([x1 + x2, x1 - x2, x2 -x1, x1**2 + x2**2, x1*x2, signed_sqrt(x1, x2)], axis = -1)"
      ],
      "metadata": {
        "id": "Q6F-kzPL36rA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_csv = pd.read_csv(ROOT + \"/submission_probabilities.csv\")\n",
        "submission_csv.head(10)"
      ],
      "metadata": {
        "id": "_Jbr3w3I4FAF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "a6e4ee32-260c-42be-b8c2-3aa28b9c4c38"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Image_ID  class  confidence      ymin      xmin      ymax      xmax  \\\n",
              "0  image_1112      0    0.956391  0.628919  0.645016  0.694816  0.790418   \n",
              "1  image_1112      0    0.944007  0.243387  0.589338  0.318815  0.763921   \n",
              "2  image_1112      0    0.917277  0.461447  0.676808  0.545236  0.786817   \n",
              "3  image_1550      0    0.984412  0.615909  0.727547  0.738454  0.872014   \n",
              "4  image_1550      0    0.971659  0.199286  0.670617  0.331255  0.805880   \n",
              "5  image_1550      0    0.956981  0.424419  0.618619  0.493475  0.813795   \n",
              "6  image_1481      0    0.974394  0.663104  0.646792  0.759310  0.860687   \n",
              "7  image_0154      0    0.986508  0.538034  0.641110  0.657833  0.886337   \n",
              "8  image_0154      0    0.983366  0.344311  0.685167  0.505868  0.858593   \n",
              "9  image_0154      0    0.933418  0.176906  0.611474  0.276579  0.824870   \n",
              "\n",
              "   vit_2_cls  vit_1_cls  resnet_2_cls  resnet_1_cls  resnet_3_cls  \\\n",
              "0   0.879022   0.942505      0.299726      0.378550      0.477408   \n",
              "1   0.079087   0.144072      0.128551      0.050557      0.043182   \n",
              "2   0.269358   0.521979      0.137847      0.096935      0.072760   \n",
              "3   0.314804   0.487395      0.749146      0.637006      0.777047   \n",
              "4   0.829947   0.954759      0.856347      0.608226      0.653994   \n",
              "5   0.192690   0.415928      0.658531      0.427732      0.263159   \n",
              "6   0.957781   0.984240      0.650876      0.556918      0.422574   \n",
              "7   0.115857   0.047401      0.148203      0.119490      0.055876   \n",
              "8   0.981118   0.982820      0.556874      0.909601      0.917879   \n",
              "9   0.058014   0.018413      0.018447      0.015657      0.003014   \n",
              "\n",
              "   senet_1_cls  senet_2_cls  senet_3_cls  \n",
              "0     0.763721     0.806535     0.752450  \n",
              "1     0.031714     0.021016     0.043715  \n",
              "2     0.208639     0.294031     0.235927  \n",
              "3     0.243104     0.273908     0.273096  \n",
              "4     0.256499     0.125517     0.273248  \n",
              "5     0.011082     0.009006     0.020303  \n",
              "6     0.967236     0.991761     0.947013  \n",
              "7     0.003809     0.005046     0.030377  \n",
              "8     0.839068     0.539289     0.883535  \n",
              "9     0.000080     0.000762     0.002412  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b564994-4eca-4e97-9f1a-461ecf9d6a4b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image_ID</th>\n",
              "      <th>class</th>\n",
              "      <th>confidence</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymax</th>\n",
              "      <th>xmax</th>\n",
              "      <th>vit_2_cls</th>\n",
              "      <th>vit_1_cls</th>\n",
              "      <th>resnet_2_cls</th>\n",
              "      <th>resnet_1_cls</th>\n",
              "      <th>resnet_3_cls</th>\n",
              "      <th>senet_1_cls</th>\n",
              "      <th>senet_2_cls</th>\n",
              "      <th>senet_3_cls</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>image_1112</td>\n",
              "      <td>0</td>\n",
              "      <td>0.956391</td>\n",
              "      <td>0.628919</td>\n",
              "      <td>0.645016</td>\n",
              "      <td>0.694816</td>\n",
              "      <td>0.790418</td>\n",
              "      <td>0.879022</td>\n",
              "      <td>0.942505</td>\n",
              "      <td>0.299726</td>\n",
              "      <td>0.378550</td>\n",
              "      <td>0.477408</td>\n",
              "      <td>0.763721</td>\n",
              "      <td>0.806535</td>\n",
              "      <td>0.752450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>image_1112</td>\n",
              "      <td>0</td>\n",
              "      <td>0.944007</td>\n",
              "      <td>0.243387</td>\n",
              "      <td>0.589338</td>\n",
              "      <td>0.318815</td>\n",
              "      <td>0.763921</td>\n",
              "      <td>0.079087</td>\n",
              "      <td>0.144072</td>\n",
              "      <td>0.128551</td>\n",
              "      <td>0.050557</td>\n",
              "      <td>0.043182</td>\n",
              "      <td>0.031714</td>\n",
              "      <td>0.021016</td>\n",
              "      <td>0.043715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>image_1112</td>\n",
              "      <td>0</td>\n",
              "      <td>0.917277</td>\n",
              "      <td>0.461447</td>\n",
              "      <td>0.676808</td>\n",
              "      <td>0.545236</td>\n",
              "      <td>0.786817</td>\n",
              "      <td>0.269358</td>\n",
              "      <td>0.521979</td>\n",
              "      <td>0.137847</td>\n",
              "      <td>0.096935</td>\n",
              "      <td>0.072760</td>\n",
              "      <td>0.208639</td>\n",
              "      <td>0.294031</td>\n",
              "      <td>0.235927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>image_1550</td>\n",
              "      <td>0</td>\n",
              "      <td>0.984412</td>\n",
              "      <td>0.615909</td>\n",
              "      <td>0.727547</td>\n",
              "      <td>0.738454</td>\n",
              "      <td>0.872014</td>\n",
              "      <td>0.314804</td>\n",
              "      <td>0.487395</td>\n",
              "      <td>0.749146</td>\n",
              "      <td>0.637006</td>\n",
              "      <td>0.777047</td>\n",
              "      <td>0.243104</td>\n",
              "      <td>0.273908</td>\n",
              "      <td>0.273096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>image_1550</td>\n",
              "      <td>0</td>\n",
              "      <td>0.971659</td>\n",
              "      <td>0.199286</td>\n",
              "      <td>0.670617</td>\n",
              "      <td>0.331255</td>\n",
              "      <td>0.805880</td>\n",
              "      <td>0.829947</td>\n",
              "      <td>0.954759</td>\n",
              "      <td>0.856347</td>\n",
              "      <td>0.608226</td>\n",
              "      <td>0.653994</td>\n",
              "      <td>0.256499</td>\n",
              "      <td>0.125517</td>\n",
              "      <td>0.273248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>image_1550</td>\n",
              "      <td>0</td>\n",
              "      <td>0.956981</td>\n",
              "      <td>0.424419</td>\n",
              "      <td>0.618619</td>\n",
              "      <td>0.493475</td>\n",
              "      <td>0.813795</td>\n",
              "      <td>0.192690</td>\n",
              "      <td>0.415928</td>\n",
              "      <td>0.658531</td>\n",
              "      <td>0.427732</td>\n",
              "      <td>0.263159</td>\n",
              "      <td>0.011082</td>\n",
              "      <td>0.009006</td>\n",
              "      <td>0.020303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>image_1481</td>\n",
              "      <td>0</td>\n",
              "      <td>0.974394</td>\n",
              "      <td>0.663104</td>\n",
              "      <td>0.646792</td>\n",
              "      <td>0.759310</td>\n",
              "      <td>0.860687</td>\n",
              "      <td>0.957781</td>\n",
              "      <td>0.984240</td>\n",
              "      <td>0.650876</td>\n",
              "      <td>0.556918</td>\n",
              "      <td>0.422574</td>\n",
              "      <td>0.967236</td>\n",
              "      <td>0.991761</td>\n",
              "      <td>0.947013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>image_0154</td>\n",
              "      <td>0</td>\n",
              "      <td>0.986508</td>\n",
              "      <td>0.538034</td>\n",
              "      <td>0.641110</td>\n",
              "      <td>0.657833</td>\n",
              "      <td>0.886337</td>\n",
              "      <td>0.115857</td>\n",
              "      <td>0.047401</td>\n",
              "      <td>0.148203</td>\n",
              "      <td>0.119490</td>\n",
              "      <td>0.055876</td>\n",
              "      <td>0.003809</td>\n",
              "      <td>0.005046</td>\n",
              "      <td>0.030377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>image_0154</td>\n",
              "      <td>0</td>\n",
              "      <td>0.983366</td>\n",
              "      <td>0.344311</td>\n",
              "      <td>0.685167</td>\n",
              "      <td>0.505868</td>\n",
              "      <td>0.858593</td>\n",
              "      <td>0.981118</td>\n",
              "      <td>0.982820</td>\n",
              "      <td>0.556874</td>\n",
              "      <td>0.909601</td>\n",
              "      <td>0.917879</td>\n",
              "      <td>0.839068</td>\n",
              "      <td>0.539289</td>\n",
              "      <td>0.883535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>image_0154</td>\n",
              "      <td>0</td>\n",
              "      <td>0.933418</td>\n",
              "      <td>0.176906</td>\n",
              "      <td>0.611474</td>\n",
              "      <td>0.276579</td>\n",
              "      <td>0.824870</td>\n",
              "      <td>0.058014</td>\n",
              "      <td>0.018413</td>\n",
              "      <td>0.018447</td>\n",
              "      <td>0.015657</td>\n",
              "      <td>0.003014</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.000762</td>\n",
              "      <td>0.002412</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b564994-4eca-4e97-9f1a-461ecf9d6a4b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1b564994-4eca-4e97-9f1a-461ecf9d6a4b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1b564994-4eca-4e97-9f1a-461ecf9d6a4b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using Features extracted by ResNet50 for Reidentification**\n",
        "Source: https://huggingface.co/microsoft/resnet-50"
      ],
      "metadata": {
        "id": "xZgyxIncRxUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"/content/Datasets/images/test\"\n",
        "suspect_path = \"/content/Datasets/images/suspects/crops\"\n",
        "\n",
        "submission_csv = pd.read_csv(ROOT + \"/submission_yolo8x.csv\") # Obtained from Object Detection Notebook"
      ],
      "metadata": {
        "id": "o9n6-V7MdfjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, processor = load_feature_extractor(\"resnet\")\n",
        "classifier = NN_Classifier(2048 * 6, 2048, 1).to(device)\n",
        "classifier.load_state_dict(torch.load(ROOT + \"/resnet_best.pt\")['model_state_dict'])\n",
        "classifier.eval()"
      ],
      "metadata": {
        "id": "zCblvyirRmnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_csv['resnet_cls'] = None\n",
        "\n",
        "for i in tqdm(range(len(submission_csv))):\n",
        "    \n",
        "  # Cropped Toy\n",
        "  img = cv2.imread(image_path + f\"/{submission_csv['Image_ID'][i]}.png\")\n",
        "  img_h, img_w = img.shape[:2]\n",
        "\n",
        "  bb = submission_csv.iloc[i]\n",
        "  \n",
        "  tl = (int(bb[\"xmin\"] * img_w), int(bb[\"ymin\"] * img_h))\n",
        "  br = (int(bb[\"xmax\"] * img_w), int(bb[\"ymax\"] * img_h))\n",
        "\n",
        "  cropped_img = img[tl[1]:br[1], tl[0]:br[0]]\n",
        "  \n",
        "  img = processor(cropped_img).reshape(1, 3, 224, 224).to(device)\n",
        "  \n",
        "  # Suspect\n",
        "  suspect_img = cv2.imread(suspect_path + f\"/{submission_csv['Image_ID'][i]}.png\")\n",
        "  suspect_img = processor(suspect_img).reshape(1, 3, 224, 224).to(device)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "      output_1 = resnet(img)['pooler_output'].reshape(1, -1)\n",
        "      output_2 = resnet(suspect_img)['pooler_output'].reshape(1, -1)\n",
        "      \n",
        "      combined_output = combine_function(output_1, output_2)\n",
        "      \n",
        "      output= classifier(combined_output)\n",
        "      y_pred = nn.Sigmoid()(output)\n",
        "      submission_csv.loc[i, \"resnet_cls\"] = y_pred.item()\n",
        "        "
      ],
      "metadata": {
        "id": "Sd5y8c24R94S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using Features extracted by Vision Transformer for Reidentification**\n",
        "Source: https://huggingface.co/google/vit-base-patch16-224"
      ],
      "metadata": {
        "id": "qRn8B4P4RCdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, processor = load_feature_extractor(\"vit\")\n",
        "classifier = NN_Classifier(768 * 6, 2048, 1).to(device)\n",
        "classifier.load_state_dict(torch.load(ROOT + \"/vit_best.pt\")['model_state_dict'])\n",
        "classifier.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRKJcZi_OS8z",
        "outputId": "90bab1f3-9655-421b-e53f-1c463e783e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NN_Classifier(\n",
              "  (linear_relu_stack): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=3072, out_features=1024, bias=True)\n",
              "    (2): GELU(approximate='none')\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=1024, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "image_path = \"/content/Datasets/images/test\"\n",
        "suspect_path = \"/content/Datasets/images/suspects\"\n",
        "\n",
        "submission_csv['vit_cls'] = None\n",
        "\n",
        "for i in tqdm(range(len(submission_csv))):\n",
        "    \n",
        "  # Cropped Toy\n",
        "  img = cv2.imread(image_path + f\"/{submission_csv['Image_ID'][i]}.png\") \n",
        "  img_h, img_w = img.shape[:2]\n",
        "\n",
        "  bb = submission_csv.iloc[i]\n",
        "  \n",
        "  tl = (int(bb[\"xmin\"] * img_w), int(bb[\"ymin\"] * img_h))\n",
        "  br = (int(bb[\"xmax\"] * img_w), int(bb[\"ymax\"] * img_h))\n",
        "\n",
        "  cropped_img = img[tl[1]:br[1], tl[0]:br[0]]\n",
        "  img = processor(BGR2RGB()(cropped_img), return_tensors = \"pt\")['pixel_values'][0].unsqueeze(0).to(device)  \n",
        "\n",
        "  # Suspect\n",
        "  suspect_img = cv2.imread(suspect_path + f\"/{submission_csv['Image_ID'][i]}.png\") \n",
        "  suspect_img = processor(BGR2RGB()(suspect_img), return_tensors = \"pt\")['pixel_values'][0].reshape(1, 3, 224, 224).to(device) \n",
        "\n",
        "  with torch.no_grad():\n",
        "      output_1 = model(img)['pooler_output'].reshape(1, -1)\n",
        "      output_2 = model(suspect_img)['pooler_output'].reshape(1, -1)\n",
        "      \n",
        "      combined_output = combine_function(output_1, output_2)\n",
        "      \n",
        "      output= classifier(combined_output)\n",
        "      y_pred = nn.Sigmoid()(output)\n",
        "      submission_csv.loc[i, \"vit_cls\"] = y_pred.item()    "
      ],
      "metadata": {
        "id": "Sxz2SGjx4HOu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4703f0c-108b-488e-9720-7c31b17cb654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3474/3474 [03:26<00:00, 16.86it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using Features extracted by SENet for Reidentification**\n",
        "Source: https://huggingface.co/docs/timm/models/se-resnet"
      ],
      "metadata": {
        "id": "PoL7pqlKQ0-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, processor = load_feature_extractor(\"senet\")\n",
        "classifier = NN_Classifier(2048 * 6, 2048, 1).to(device)\n",
        "classifier.load_state_dict(torch.load(ROOT + \"/senet_best.pt\")['model_state_dict'])\n",
        "classifier.eval()"
      ],
      "metadata": {
        "id": "6XmG0MtRQuA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "image_path = \"/content/Datasets/images/test\"\n",
        "suspect_path = \"/content/Datasets/images/suspects\"\n",
        "\n",
        "submission_csv['senet_cls'] = None\n",
        "\n",
        "for i in tqdm(range(len(submission_csv))):\n",
        "    \n",
        "  # Cropped Toy\n",
        "  img = cv2.imread(image_path + f\"/{submission_csv['Image_ID'][i]}.png\") \n",
        "  img_h, img_w = img.shape[:2]\n",
        "\n",
        "  bb = submission_csv.iloc[i]\n",
        "  \n",
        "  tl = (int(bb[\"xmin\"] * img_w), int(bb[\"ymin\"] * img_h))\n",
        "  br = (int(bb[\"xmax\"] * img_w), int(bb[\"ymax\"] * img_h))\n",
        "\n",
        "  cropped_img = img[tl[1]:br[1], tl[0]:br[0]]\n",
        "\n",
        "  cropped_img = Image.fromarray(BGR2RGB()(cropped_img)) \n",
        "  \n",
        "  img = processor(cropped_img).unsqueeze(0).to(device) \n",
        "\n",
        "  # Suspect\n",
        "  suspect_img = Image.open(suspect_path + f\"/{submission_csv['Image_ID'][i]}.png\").convert('RGB')\n",
        "  \n",
        "  suspect_img = processor(suspect_img).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "  with torch.no_grad():\n",
        "      output_1 = model(img).reshape(1, -1)\n",
        "      output_2 = model(suspect_img).reshape(1, -1)\n",
        "      \n",
        "      combined_output = combine_function(output_1, output_2)\n",
        "      \n",
        "      output= classifier(combined_output)\n",
        "      y_pred = nn.Sigmoid()(output)\n",
        "      print(y_pred)\n",
        "      submission_csv.loc[i, \"senet_cls\"] = y_pred.item()\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93YR_-aO_2qg",
        "outputId": "9d536719-3ebc-4c15-dce6-498d294c4151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3474 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2123]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using Combined Features extracted by ResNet, SENet, and Vision Transformer for Reidentification**"
      ],
      "metadata": {
        "id": "7MpQ8PmSQcFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model, resnet_processor = load_feature_extractor(\"resnet\")\n",
        "senet_model, senet_processor = load_feature_extractor(\"senet\")\n",
        "vit_model, vit_processor = load_feature_extractor(\"vit\")\n",
        "classifier = NN_Classifier(4864 * 6, 1024, 1).to(device)\n",
        "classifier.load_state_dict(torch.load(ROOT + \"/combined_best.pt\")['model_state_dict'])\n",
        "classifier.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqLujSTS4tpp",
        "outputId": "c43dc5b4-e61b-420e-ded8-c8f762b75539"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/resnet-50 were not used when initializing ResNetModel: ['classifier.1.weight', 'classifier.1.bias']\n",
            "- This IS expected if you are initializing ResNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ResNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NN_Classifier(\n",
              "  (linear_relu_stack): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=29184, out_features=1024, bias=True)\n",
              "    (2): GELU(approximate='none')\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=1024, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "image_path = \"/content/Datasets/images/test\"\n",
        "suspect_path = \"/content/Datasets/images/suspects\"\n",
        "\n",
        "submission_csv['combined_cls'] = None\n",
        "\n",
        "for i in tqdm(range(len(submission_csv))):\n",
        "    \n",
        "  # Cropped Toy\n",
        "  img = cv2.imread(image_path + f\"/{submission_csv['Image_ID'][i]}.png\") \n",
        "  img_h, img_w = img.shape[:2]\n",
        "\n",
        "  bb = submission_csv.iloc[i]\n",
        "  \n",
        "  tl = (int(bb[\"ymin\"] * img_w), int(bb[\"xmin\"] * img_h))\n",
        "  br = (int(bb[\"ymax\"] * img_w), int(bb[\"xmax\"] * img_h))\n",
        "\n",
        "  cropped_img = img[tl[1]:br[1], tl[0]:br[0]]\n",
        "\n",
        "\n",
        "  resnet_cropped_img = resnet_processor(cropped_img).unsqueeze(0).to(device)\n",
        "\n",
        "  vit_cropped_img = Image.fromarray(BGR2RGB()(cropped_img)) \n",
        "  vit_cropped_img = vit_processor(vit_cropped_img, return_tensors = \"pt\")['pixel_values'][0].unsqueeze(0).to(device)\n",
        "\n",
        "  senet_cropped_img = Image.fromarray(BGR2RGB()(cropped_img)) \n",
        "  senet_cropped_img = senet_processor(senet_cropped_img).unsqueeze(0).to(device)\n",
        "\n",
        "  # Suspect\n",
        "  resnet_suspect_img = cv2.imread(suspect_path + f\"/{submission_csv['Image_ID'][i]}.png\") \n",
        "  resnet_suspect_img = resnet_processor(resnet_suspect_img).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "  vit_suspect_img = Image.open(suspect_path + f\"/{submission_csv['Image_ID'][i]}.png\").convert('RGB')\n",
        "  vit_suspect_img = vit_processor(vit_suspect_img, return_tensors = \"pt\")['pixel_values'][0].unsqueeze(0).to(device)\n",
        "\n",
        "  senet_suspect_img = Image.open(suspect_path + f\"/{submission_csv['Image_ID'][i]}.png\").convert('RGB')\n",
        "  senet_suspect_img = senet_processor(senet_suspect_img).unsqueeze(0).to(device)  \n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "      resnet_output_1 = resnet_model(resnet_cropped_img)['pooler_output'].reshape(1,-1)\n",
        "      resnet_output_2 = resnet_model(resnet_suspect_img)['pooler_output'].reshape(1,-1)\n",
        "\n",
        "      vit_output_1 = vit_model(vit_cropped_img)['pooler_output'].reshape(1, -1)\n",
        "      vit_output_2 = vit_model(vit_suspect_img)['pooler_output'].reshape(1, -1)\n",
        "      \n",
        "      senet_output_1 = senet_model(senet_cropped_img).reshape(1,-1)\n",
        "      senet_output_2 = senet_model(senet_suspect_img).reshape(1,-1)\n",
        "\n",
        "      output_1 = torch.concat([resnet_output_1, senet_output_1, vit_output_1], dim = -1)\n",
        "      output_2 = torch.concat([resnet_output_2, senet_output_2, vit_output_2], dim = -1)\n",
        "      \n",
        "      combined_output = combine_function(output_1, output_2)\n",
        "      \n",
        "      output= classifier(combined_output)\n",
        "      y_pred = nn.Sigmoid()(output)\n",
        "      submission_csv.loc[i, \"combined_cls\"] = y_pred.item()\n",
        "  \n",
        "    "
      ],
      "metadata": {
        "id": "X5Qk6saU4ubS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Simple Ensemble**"
      ],
      "metadata": {
        "id": "mdlkz8LbTelf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(submission_csv)):\n",
        "    if (submission_csv[\"resnet_cls\"][i] \n",
        "        + submission_csv[\"senet_cls\"][i] \n",
        "        + submission_csv[\"vit_cls\"][i]\n",
        "        + 3 * submission_csv[\"combined_cls\"][i]\n",
        "       )/6 > 0.5:\n",
        "        submission_csv.loc[i, \"class\"] = 1\n",
        "    else:\n",
        "        submission_csv.loc[i, \"class\"] = 0"
      ],
      "metadata": {
        "id": "7gaXAJh_Ye6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_csv = submission_yolo8x.drop(columns = [\"resnet_cls\", \"senet_cls\", \"vit_cls\", \"combined_cls\"])\n",
        "submission_csv.to_csv(ROOT + \"/submission_combined2_311.csv\", index = False)"
      ],
      "metadata": {
        "id": "7UizXx5tSZ1T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}